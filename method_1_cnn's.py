# -*- coding: utf-8 -*-
"""Method 1 - CNN's.ipynb

Automatically generated by Colab.

Method 1 - Convolutional Neural Networks
"""

import tensorflow as tf
import os
import cv2
import imghdr
import numpy as np
import keras
from google.colab.patches import cv2_imshow
from matplotlib import pyplot as plt
from keras.utils import normalize, to_categorical
from keras.optimizers import RMSprop, Adam
from keras.layers import Dense
import pandas as pd
from pathlib import Path
import sklearn
import os.path
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
from tensorflow.keras import layers
import tensorflow_datasets as tfds
from keras.applications.resnet50 import ResNet50
from keras.applications.resnet50 import preprocess_input, decode_predictions

type_image_directory = Path('insert path')
type_dataset = tf.keras.utils.image_dataset_from_directory(type_image_directory, batch_size=18)
shape_image_directory = Path('insert path')
shape_dataset = tf.keras.utils.image_dataset_from_directory(shape_image_directory, batch_size=18)
voltage_image_directory = Path('insert path')
voltage_dataset = tf.keras.utils.image_dataset_from_directory(voltage_image_directory, batch_size=18)

def resize_images(image, label):
  image = tf.image.resize(image, (224,224))
  return image, label

def one_hot_encode(image, label):
  num_classes = 4
  return image, tf.one_hot(label, depth=num_classes)

def voltage_one_hot_encode(image, label):
  num_classes = 2
  return image, tf.one_hot(label, depth=num_classes)

type_data = type_dataset.map(lambda x,y: (x/255, y))
type_scaled_iterator = type_data.as_numpy_iterator()
type_batch = type_scaled_iterator.next()

voltage_data = voltage_dataset.map(lambda x,y: (x/255, y))
voltage_scaled_iterator = voltage_data.as_numpy_iterator()
voltage_batch = voltage_scaled_iterator.next()

shape_data = shape_dataset.map(lambda x,y: (x/255, y))
shape_scaled_iterator = shape_data.as_numpy_iterator()
shape_batch = shape_scaled_iterator.next()

def type_split():
  global type_train
  global type_val
  global type_test
  type_train_size = 'insert size'
  type_val_size = 'insert size'
  type_test_size = 'insert size'
  type_train = type_data.take(type_train_size)
  type_train = type_train.map(resize_images)
  type_train = type_train.map(one_hot_encode)
  type_train = type_train.repeat() #if not enough images, repeat training data
  type_val = type_data.skip(type_train_size).take(type_val_size)
  type_val = type_val.map(resize_images)
  type_val = type_val.map(one_hot_encode)
  type_test = type_data.skip(type_train_size + type_val_size).take(type_test_size)
  type_test = type_test.map(resize_images)
  type_test = type_test.map(one_hot_encode)

#can modify hyperparameters
type_split()
type_activation = 'sigmoid'
type_model = tf.keras.applications.ResNet50(include_top=False, weights='imagenet')
type_x = type_model.output
type_x = tf.keras.layers.GlobalAveragePooling2D()(type_x)
type_x = Dense(10, activation='relu')(type_x)
type_prediction = tf.keras.layers.Dense(4, activation='softmax')(type_x)
type_model = tf.keras.Model(inputs = type_model.input, outputs = type_prediction)
adam = Adam(learning_rate=0.001)
type_model.compile(optimizer=adam,loss = 'categorical_crossentropy', metrics = ['accuracy'])
type_history = type_model.fit(type_train, epochs = 40, steps_per_epoch = 20, validation_data = type_val)

num_epochs = range(1, 41) #change this depending on number of epochs
accuracy = type_history.history['accuracy']
val_accuracy = type_history.history['val_accuracy']
plt.plot(num_epochs, accuracy, 'g', label='Training accuracy')
plt.plot(num_epochs, val_accuracy, 'r', label='Validation accuracy')
plt.title('Type of Material\nTraining and Validation Accuracy', fontsize = 18)
plt.xlabel('Epochs', fontsize = 15)
plt.ylabel('Accuracy', fontsize = 15)
plt.legend()
plt.show()

def voltage_split():
  global voltage_train
  global voltage_val
  global voltage_test
  voltage_train_size = 'insert size'
  voltage_val_size = 'insert size'
  voltage_test_size = 'insert size'
  voltage_train = voltage_data.take(voltage_train_size)
  voltage_train = voltage_train.map(resize_images)
  voltage_train = voltage_train.map(voltage_one_hot_encode)
  voltage_train = voltage_train.repeat() #not enough images, so repeat training data
  voltage_val = voltage_data.skip(voltage_train_size).take(voltage_val_size)
  voltage_val = voltage_val.map(resize_images)
  voltage_val = voltage_val.map(voltage_one_hot_encode)
  voltage_test = voltage_data.skip(voltage_train_size + voltage_val_size).take(voltage_test_size)
  voltage_test = voltage_test.map(resize_images)
  voltage_test = voltage_test.map(voltage_one_hot_encode)

voltage_split()
voltage_activation = 'sigmoid'
voltage_model = tf.keras.applications.ResNet50(include_top=False, weights='imagenet')
voltage_x = voltage_model.output
voltage_x = tf.keras.layers.GlobalAveragePooling2D()(voltage_x)
voltage_x = Dense(1024, activation='relu')(voltage_x)
voltage_prediction = tf.keras.layers.Dense(2, activation='softmax')(voltage_x)
voltage_model = tf.keras.Model(inputs = voltage_model.input, outputs = voltage_prediction)
voltage_model.compile(optimizer = 'adam',loss = 'categorical_crossentropy', metrics = ['accuracy'])
voltage_history = voltage_model.fit(voltage_train, steps_per_epoch=40, epochs = 15, validation_data = voltage_val)

num_epochs = range(1, 16)
accuracy = voltage_history.history['accuracy']
val_accuracy = voltage_history.history['val_accuracy']
plt.plot(num_epochs, accuracy, 'g', label='Training accuracy')
plt.plot(num_epochs, val_accuracy, 'r', label='Validation accuracy')
plt.title('Operating Voltage\nTraining and Validation Accuracy', fontsize = 18)
plt.xlabel('Epochs', fontsize = 15)
plt.ylabel('Accuracy', fontsize = 15)
plt.legend()
plt.show()

def shape_split():
  global shape_train
  global shape_val
  global shape_test
  shape_train_size = 7
  shape_val_size = 1
  shape_test_size = 1
  shape_train = shape_data.take(shape_train_size)
  shape_train = shape_train.map(resize_images)
  shape_train = shape_train.map(one_hot_encode)
  shape_train = shape_train.repeat() #not enough images, so repeat training data
  shape_val = shape_data.skip(shape_train_size).take(shape_val_size)
  shape_val = shape_val.map(resize_images)
  shape_val = shape_val.map(one_hot_encode)
  shape_test = shape_data.skip(shape_train_size + shape_val_size).take(shape_test_size)
  shape_test = shape_test.map(resize_images)
  shape_test = shape_test.map(one_hot_encode)

shape_split()
shape_activation = 'sigmoid'
shape_model = tf.keras.applications.ResNet50(include_top=False, weights='imagenet')
shape_x = shape_model.output
shape_x = tf.keras.layers.GlobalAveragePooling2D()(shape_x)
shape_x = Dense(1024, activation='relu')(shape_x)
shape_prediction = tf.keras.layers.Dense(4, activation='softmax')(shape_x)
shape_model = tf.keras.Model(inputs = shape_model.input, outputs = shape_prediction)
shape_model.compile(optimizer = 'adam',loss = 'categorical_crossentropy', metrics = ['accuracy'])
shape_history = shape_model.fit(shape_train, steps_per_epoch =20, epochs =40, validation_data = shape_val)

num_epochs = range(1, 41)
accuracy = shape_history.history['accuracy']
val_accuracy = shape_history.history['val_accuracy']
plt.plot(num_epochs, accuracy, 'g', label='Training accuracy')
plt.plot(num_epochs, val_accuracy, 'r', label='Validation accuracy')
plt.title('Shape\nTraining and Validation Accuracy', fontsize = 18)
plt.xlabel('Epochs', fontsize = 15)
plt.ylabel('Accuracy', fontsize = 15)
plt.legend()
plt.show()

#change to output desired categories
def identify_material(yhat):
  global type_of_material
  Cr = yhat[0][0]
  Fe = yhat[0][1]
  NU1000 = yhat[0][2]
  UiO = yhat[0][3]

  print("Percent MIL-101 Cr: ", Cr*100, "%")
  print("Percent MIL-101 Fe: ", Fe*100, "%")
  print("Percent NU-1000: ", NU1000*100, "%")
  print("Percent UiO-66: ", UiO*100, "%")

  if (Cr == max(Cr,Fe,NU1000,UiO)):
    type_of_material = 'MOF MIL-101 Cr'
  elif (Fe == max(Cr,Fe,NU1000,UiO)):
   type_of_material = 'MOF MIL-101 Fe'
  elif (NU1000 == max(Cr,Fe,NU1000,UiO)):
    type_of_material = 'MOF NU-1000'
  else:
    type_of_material = 'MOF UiO-66'

#change to output desired categories
def identify_shape(yhat):
  global shape
  cubic = yhat[0][0]
  octahedral = yhat[0][1]
  pyramidal = yhat[0][2]
  rod = yhat[0][3]

  print("Percent Cubic: ", cubic*100, "%")
  print("Percent Octahedral: ", octahedral*100, "%")
  print("Percent Pyramidal: ", pyramidal*100, "%")
  print("Percent Rod: ", rod*100, "%")

  if (cubic == max(cubic,octahedral,pyramidal,rod)):
    shape = 'cubic'
  elif (octahedral == max(cubic,octahedral,pyramidal,rod)):
    shape = 'octahedral'
  elif (pyramidal == max(cubic,octahedral,pyramidal,rod)):
    shape = 'pyramidal'
  else:
    shape = 'rod-like'

#change to output desired categories
def identify_voltage(yhat):
  global operating_voltage
  opvol1 = yhat[0][0]
  opvol2 = yhat[0][1]

  print("Percent 5 kV: ", opvol1*100, "%")
  print("Percent 15 kV: ", opvol2*100, "%")

  if (opvol1 == max(opvol1, opvol2)):
    operating_voltage = '5 kV'
  else:
    operating_voltage = '15 kV'

text_directory = Path('drive/MyDrive/All Directories/Magnification/Text')
magnifications_initial=[]
ordered_files=[]
for i in range(len(os.listdir(text_directory))+1):
  if i == 0:
    continue
  if (i<=103):
    name = str(i)+"File.TXT"
    name2 = str(i)+"File.jpeg"
    ordered_files.append(name2)
  if (i>103):
    name = str(i)+"File .TXT"
    name2 = str(i)+"File .jpeg"
    ordered_files.append(name2)
  with open(os.path.join(text_directory, name)) as f:
    for line in f:
      if ('Magnification' in line and "SubMagnification" not in line):
        mag = float(line.split('=')[1])
        magnifications_initial.append(mag)

magnification_imagepaths = pd.Series(ordered_files, name='Filepath').astype(str)
magnifications_initial_logged = np.log10(magnifications_initial)
magnifications = pd.Series(magnifications_initial_logged, name='Magnification')
magnification_images = pd.concat([magnification_imagepaths, magnifications], axis=1).sample(frac=1.0, random_state=1).reset_index(drop=True)

def magnification_split():
  global magnification_train_images
  global magnification_val_images
  global magnification_test_images
  train_df, test_df = train_test_split(magnification_images, train_size=0.8, shuffle=True, random_state=1)
  train_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, validation_split=0.2)
  test_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)
  train_df['Filepath'] = train_df['Filepath'].apply(lambda x: '/content/drive/MyDrive/All Directories/Magnification/MagnificationCropped/' + x)
  test_df['Filepath'] = test_df['Filepath'].apply(lambda x: '/content/drive/MyDrive/All Directories/Magnification/MagnificationCropped/' + x)
  magnification_train_images = train_generator.flow_from_dataframe(dataframe = train_df, x_col='Filepath', y_col='Magnification', target_size=(120, 120), class_mode='raw', batch_size=32, shuffle=True, subset='training')
  magnification_val_images = train_generator.flow_from_dataframe(dataframe = train_df, x_col='Filepath', y_col='Magnification', target_size=(120, 120), class_mode='raw', batch_size=32, shuffle=True, subset='validation')
  magnification_test_images = test_generator.flow_from_dataframe(dataframe = test_df, x_col='Filepath', y_col='Magnification', target_size=(120, 120), class_mode='raw', batch_size=32, shuffle=False)

magnification_split()
inputs = tf.keras.Input(shape=(120, 120, 3))
x = tf.keras.layers.Conv2D(filters=16, kernel_size=(3,3), activation='relu')(inputs)
x = tf.keras.layers.MaxPooling2D()(x)
x = tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu')(x)
x=tf.keras.layers.Flatten()(x)
x = tf.keras.layers.Dense(64, activation='relu')(x)
outputs = tf.keras.layers.Dense(1)(x)
magnification_model = tf.keras.Model(inputs=inputs, outputs=outputs)
magnification_model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.002), loss='mse', metrics = ['mse'])
magnification_history = magnification_model.fit(magnification_train_images, validation_data = magnification_val_images, epochs=60)

num_epochs = range(1, 61)
mse = magnification_history.history['mse']
val_mse = magnification_history.history['val_mse']
plt.plot(num_epochs, mse, 'g', label='Training MSE')
plt.plot(num_epochs, val_mse, 'r', label='Validation MSE')
plt.title('Magnification\nTraining and Validation MSE', fontsize = 18)
plt.xlabel('Epochs', fontsize = 15)
plt.ylabel('MSE', fontsize = 15)
plt.ylim((0,20))
plt.legend()
plt.show()

image = cv2.imread('input path to image')
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
magnification_image = tf.image.resize(image, (120,120))
plt.imshow(image.astype(int))
plt.show()

type_prediction = type_model.predict(np.expand_dims(image/255,0))
identify_material(type_prediction)
voltage_prediction = voltage_model.predict(np.expand_dims(image/255,0))
identify_voltage(voltage_prediction)
shape_prediction = shape_model.predict(np.expand_dims(image/255,0))
identify_shape(shape_prediction)
magnification_logged = np.squeeze(magnification_model.predict(np.expand_dims(magnification_image/255, 0)))
magnification = 10**magnification_logged

def descriptor():
  print(f'This material is a {type_of_material} with a shape that is {shape}. The image was taken at an operating voltage of {operating_voltage} and a magnification of {magnification}.')

descriptor()
